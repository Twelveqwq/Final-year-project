{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c301e46-bb1b-421b-9bbb-5c524b3ba1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#决策树节点类\n",
    "class decisionNode:\n",
    "    #Impurity是一个广义的概念，用于表示数据集的混合程度和不确定性程度\n",
    "    def __init__(self, feature, threshold, left, right, label, gini, is_leaf):\n",
    "        self.feature = feature           #划分特征的维度\n",
    "        self.threshold = threshold       #划分阈值\n",
    "        self.left = left                 #左子树\n",
    "        self.right = right               #右子树\n",
    "        self.label = label               #叶节点值（标签）\n",
    "        self.is_leaf = is_leaf             #是否为叶节点\n",
    "        self.gini = gini                 #储存该节点的Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa22ef3f-1bb3-4f86-9538-92d807bf6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getClasses(label):\n",
    "    # 如果标签为空，则返回0\n",
    "    if not label:\n",
    "        return 0\n",
    "    unique_classes = set(label.flatten())  # 使用 set 来获取不同类别\n",
    "    num = len(unique_classes)\n",
    "    return num\n",
    "\n",
    "\n",
    "#计算叶节点值\n",
    "def get_label(dataset):\n",
    "    labels = dataset[:,-1]\n",
    "    #初始化一个字典\n",
    "    label_counts = {}\n",
    "    #键是数据集中出现的类别，而值是对应类别出现的次数。\n",
    "    for label in labels:                    #遍历数据集中的每个样本的标签\n",
    "        if label in label_counts:           #检查当前标签是否已存在于字典中\n",
    "            label_counts[label] += 1        #如果已存在，对应值加一，表示该类别出现的次数加一\n",
    "        else:\n",
    "            label_counts[label] = 1         #如果不存在，则添加该标签，并初始化值为一，表示该类别第一次出现\n",
    "    \n",
    "    max_count = max(label_counts.values())  #得到数据集中出现次数最多的类别的次数\n",
    "    \n",
    "    #‘largest_labels存储所有出现次数最多的类别’\n",
    "    #循环遍历 label_counts 字典中的每个键值pair，\n",
    "    #如果出现次数等于 max_count，则将对应的类别标签 label 添加到列表中。\n",
    "    largest_labels = [label for label, count in label_counts.items() \n",
    "                      if count == max_count]\n",
    "    #largest_labels 是存储了出现次数最多的类别的列表。\n",
    "    #因为有可能存在多个类别出现次数相同的情况，所以 largest_labels 中可能包含了多个类别。\n",
    "    \n",
    "    #从出现次数最多的类别中选择第一个类别作为叶节点值\n",
    "    #目的是确保每次运行算法时得到相同的结果，这样可以保持算法的可重复性和稳定性。\n",
    "    return largest_labels[0]\n",
    "\n",
    "\n",
    "#定义计算gini值的函数\n",
    "def getGini(dataset):\n",
    "\n",
    "    #dataset: the combination of data and label\n",
    "    #dataset的最后一列是标签\n",
    "    datalength = dataset.shape[0]       # dataset数据点的长度     \n",
    "    #使用np.bincount得到每个类别出现的次数\n",
    "    #label = dataset[:,-1]\n",
    "    counts = np.bincount(dataset[:,-1].astype(int))\n",
    "    probs  = counts / datalength        # 计算每个类别出现的概率\n",
    "    gini = 1 - np.sum(probs ** 2)\n",
    "    return gini\n",
    "\n",
    "\n",
    "#划分用函数，用于划分后寻找最小gini值的划分阈值\n",
    "def splitData(dataset, feature, threshold, type):\n",
    "    datalength = dataset.shape[0]       #记录数据总长度\n",
    "    channel = feature                   #当前划分特征维度\n",
    "\n",
    "    subdataset = []                     #创建子数据集的空例，为后续储存数据做准备\n",
    "    if type == 0:\n",
    "        for i in range(datalength):\n",
    "            #使用float函数确保比较的值是浮点数，而不是字符串或其他数据类型\n",
    "            if float(dataset[i,feature]) <= threshold:      #储存小于等于阈值的数据为子数据集\n",
    "                subdataset.append(dataset[i,:])                #储存整行数据\n",
    "    elif type == 1:\n",
    "        for i in range(datalength):\n",
    "            if float(dataset[i,feature]) >  threshold :      #储存大于阈值的数据为子数据集\n",
    "                subdataset.append(dataset[i,:])                #储存整行数据\n",
    "\n",
    "    return np.array(subdataset), len(subdataset)\n",
    "\n",
    "#找到最佳特征列\n",
    "def bestFeature(dataset):\n",
    "\n",
    "    datalength = dataset.shape[0]       #记录数据总长度\n",
    "    numFeatures = len(dataset[0]) - 1   #特征总数，减一是因为dataset最后一列是标签\n",
    "    #当只有一个特征时\n",
    "    #if dataset.size < 0:\n",
    "    #    bestFeature = 0\n",
    "    #    return bestFeature\n",
    "        \n",
    "    #预设定gini值\n",
    "    minGini = 1\n",
    "    bestThreshold = []                      #创建空例，储存阈值\n",
    "    bestFeature = []                        #创建空例，储存特征维度\n",
    "    #start_time = time.time()\n",
    "\n",
    "    for i in range(numFeatures):        #从第i个特征开始遍历所有特征\n",
    "        feaColumn = list(dataset[:,i])  #提取第i个维度的数据列，并转化为列表，为了方便排序\n",
    "        sorted_data = sorted(feaColumn) #对这列特征排序\n",
    "        feaGini = 1                     #预设定该特征下使用阈值划分计算得到的最小gini值为1\n",
    "        feaThres = []                   #创建空例，储存该特征下每个gini值对应的划分阈值\n",
    "        \n",
    "        for j in range(datalength - 1): #循环的目的是为了寻找gini值最小的划分阈值\n",
    "            threshold = (sorted_data[j]+sorted_data[j+1])/2\n",
    "            dataLeft , lenLeft  = splitData(dataset, i, threshold, 0)      #当前特征下，划分阈值划分的左子树\n",
    "            dataRight, lenRight = splitData(dataset, i, threshold, 1)      #右子树 \n",
    "            # 检查子数据集是否为空，如果是，则跳过当前阈值\n",
    "            if lenLeft == 0 or lenRight == 0:\n",
    "                continue\n",
    "            #计算此分法对应Gini值\n",
    "            GiniValue = (lenLeft/datalength) * getGini(dataLeft) + (lenRight/datalength) * getGini(dataRight)  \n",
    "\n",
    "            if GiniValue < feaGini:   #如果新计算的gini值小于与预设值，更新gini和阈值\n",
    "                feaGini = GiniValue    #储存这一循环中，划分阈值下计算的gini值\n",
    "                feaThre = threshold    #储存这一循环中，划分阈值\n",
    "            \n",
    "        if feaGini < minGini:\n",
    "            minGini = feaGini          #储存这一特征下最小的gini值\n",
    "            bestThreshold = feaThre    #储存这一特征下，最小gini值对应的划分阈值\n",
    "            bestFeature = i            #储存这一特征维度\n",
    "        \n",
    "    #print('特征：',bestFeature)   \n",
    "    #print('一个划分特征所用时间 %fs!' % (time.time() - start_time))\n",
    "    return bestFeature, bestThreshold, minGini\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#创建决策树\n",
    "def build_tree(dataset, depth, max_depth,min_samples_split):\n",
    "    \n",
    "    length = dataset.shape[0]\n",
    "    max_depth = max_depth\n",
    "    min_samples_split = min_samples_split\n",
    "    #if depth == self.max_depth or length < self.min_samples_split:\n",
    "    #    return node\n",
    "    if dataset.size<=0:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    #得到最好的划分特征维度和划分阈值，以及对应的最小gini值\n",
    "    feature, threshold, minGini = bestFeature(dataset)\n",
    "    \n",
    "    #按照划分特征的维度进行排序\n",
    "    sorted_data = sorted(dataset, key=lambda x: x[feature])  \n",
    "\n",
    "    \n",
    "    #根据划分特征维度和阈值划分左右子树\n",
    "    left_data = np.array([row for row in sorted_data if row[feature] <= threshold])\n",
    "    right_data = np.array([row for row in sorted_data if row[feature] > threshold])\n",
    "    leftLen = left_data.shape[0]\n",
    "    rightLen = right_data.shape[0]\n",
    "    #print ('左子树数据形状 ' , (leftLen))\n",
    "    #print ('右子树数据形状 ' , (rightLen))\n",
    "    left = None\n",
    "    right = None\n",
    "    #判断是否达到最大深度，或节点中的样本数是否小于min_samples_split时\n",
    "    if depth < max_depth and leftLen > 1:\n",
    "        # or length >= self.min_samples_split:\n",
    "        #递归构建左右子树\n",
    "        left = build_tree(left_data, depth + 1,\n",
    "                          max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    if depth < max_depth and rightLen >1:    \n",
    "        right = build_tree(right_data, depth + 1,\n",
    "                          max_depth=max_depth,min_samples_split=min_samples_split)\n",
    "    #创建内部节点\n",
    "    node = decisionNode(feature=feature, threshold=threshold, gini = minGini,\n",
    "                        left =left, right = right, label = None,is_leaf=None)\n",
    "    \n",
    "    #判断是否达到最大深度，或节点中的样本数小于min_samples_split时，返回叶子节点的值\n",
    "    if depth == max_depth or length <= min_samples_split:\n",
    "\n",
    "        label = get_label(dataset)  # 获取叶节点值\n",
    "        node.label = label\n",
    "        node.is_leaf = True\n",
    "    \n",
    "    return node\n",
    "\n",
    "\n",
    "#分类树类\n",
    "#class Classifier:\n",
    "\n",
    "#    def __init__(self):\n",
    "        #用数据集构建决策树\n",
    "\n",
    "        #self.min_samples_split = min_samples_split\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #决策树搜索\n",
    "def searchTree(tree, point):\n",
    "\n",
    "    #检查决策树是否存在，否则不执行\n",
    "    if tree is None:                              \n",
    "        return None\n",
    "    \n",
    "    #用于记录搜索路径上的节点信息\n",
    "    stack = []\n",
    "\n",
    "    #在决策树中搜索，并确定point会落在哪个叶子节点上\n",
    "    current_node = tree                                         #储存根节点；决策树结构为根节点下左右子节点树，下一个循环后目标点更新成左节点或右节点，一直循环，直到目标点为none\n",
    "\n",
    "    while current_node:                                              #当前节点根据循环不停向下递归查找最邻近的点，直到提取数据点为None结束循环\n",
    "        stack.append(current_node)                                   #当前节点信息存储于堆栈中\n",
    "        threshold = current_node.threshold                           #从当前节点中提取阈值\n",
    "        feature = current_node.feature                               #从当前节点中提取划分特征维度\n",
    "        \n",
    "        if current_node.is_leaf == True:                             #判断当前节点是否为叶子节点\n",
    "            return current_node.label                                #如果是，返回当前叶节点值为预测标签\n",
    "        \n",
    "        if point.size > 0 and point[feature] <= threshold:                            #比较当前维度下未知点和阈值的大小，判断走左子树还是右子树\n",
    "            current_node = current_node.left\n",
    "        else:\n",
    "            current_node = current_node.right\n",
    "\n",
    "def selectSamples(data, label, numberSamplePerClass):\n",
    "\n",
    "    unique_classes = np.unique(label)  # 获取唯一的类别标签\n",
    "    #初始化抽样后的数据和标签列表\n",
    "    subData = []\n",
    "    subLabel = []\n",
    "    num = numberSamplePerClass\n",
    "\n",
    "    #对每个类别进行遍历\n",
    "    for i in unique_classes:\n",
    "        #找到标签中当前类别的索引\n",
    "        index = np.where(label == i)[0]\n",
    "        \n",
    "        #处理样本不足的情况，可以选择跳过该类别或者使用全部样本\n",
    "        if len(index) < num:\n",
    "            Idx = len(index)  # 使用全部样本\n",
    "            selectedIdx = np.random.choice(index, size=Idx, replace=True)\n",
    "        else:    \n",
    "            #从当前类别的索引中随机抽样指定数量的样本,replace参数为True意味是有放回抽样\n",
    "            selectedIdx = np.random.choice(index, size=num, replace=True)\n",
    "        #将抽样后的数据和标签添加到列表中\n",
    "        subData.extend(data[selectedIdx,:])\n",
    "        subLabel.extend(label[selectedIdx])\n",
    "    \n",
    "            #将抽样后的数据和标签转换为numpy数组并返回\n",
    "    subdata = np.array(subData)\n",
    "    sublabel = np.array(subLabel)\n",
    "    sublabel = sublabel.reshape([-1,1])\n",
    "    #print(subdata.shape)\n",
    "    #print(sublabel.shape)\n",
    "\n",
    "    #将抽样后的数据和标签转换为numpy数组并返回\n",
    "    return subdata, sublabel\n",
    "        \n",
    "#随机森林类\n",
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, num_estimators, max_depth, min_samples_split, numberSamplePerClass):\n",
    "        self.num_estimators = num_estimators           #储存决策树数量\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []                                #存储随机森林中的决策树\n",
    "        self.numberSamplePerClass = numberSamplePerClass\n",
    "        \n",
    "\n",
    "    #用数据构建决策树\n",
    "    def fit(self, data, label): \n",
    "        count=0\n",
    "        for _ in range(self.num_estimators):    #循环使用下划线因为不关心循环变量的值，只是想重复执行创建决策树的操作\n",
    "            #从数据集中有放回地随机抽样作为决策树训练数据\n",
    "            start_time = time.time()\n",
    "            subdata, sublabel = selectSamples(data, label, self.numberSamplePerClass)\n",
    "            #将数据集和标签结合在一起\n",
    "            dataset = np.hstack((subdata, sublabel)) \n",
    "            count = count+1\n",
    "            print(count)\n",
    "            subtree = build_tree(dataset, depth=0,max_depth=self.max_depth, min_samples_split=self.min_samples_split)            #构建决策树\n",
    "            self.trees.append(subtree)             #储存决策树\n",
    "            print('一棵树生成所用时间 %fs!' % (time.time() - start_time))\n",
    "            \n",
    "            \n",
    "    #预测\n",
    "    def predict(self, testData):\n",
    "        #（暂时不用）datalength = testData.shape[0]\n",
    "        predictions = []  #用于存储每个树的预测结果\n",
    "        for point in testData:\n",
    "            pointpredictions = []  #储存每棵树对该点的预测\n",
    "            for tree in self.trees:#按循环以此使用每棵树\n",
    "                prediction = searchTree(tree,point)\n",
    "                pointpredictions.append(prediction)\n",
    "\n",
    "            #对于每个数据点，返回多个决策树的结果投票得出预测值\n",
    "            # set(pointpredictions)作用是去重，这样可以得到一个不重复的元素集合\n",
    "            # max()函数结合key=pointpredictions.count参数可以找到在预测点中出现次数最多的元素\n",
    "            label = max(set(pointpredictions), key=pointpredictions.count)\n",
    "            predictions.append(label)\n",
    "\n",
    "        #将预测结果列表转换为NumPy数组后，reshape成二维数组，方便之后与正确标签进行对比\n",
    "        predictions_array = np.array(predictions)\n",
    "        predictions_reshaped = np.reshape(predictions_array, (-1, 1))\n",
    "\n",
    "        return predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf09804e-f882-45a7-ac8a-126ece4cafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47f9bafa-3ad8-4e0c-a0e7-f842131676a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9531,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "file = h5py.File('DB2//DB2_S1_feature_200_0.h5','r')\n",
    "featureData   = file['featureData'][:]\n",
    "featureLabel  = file['featureLabel'][:]\n",
    "file.close()\n",
    "\n",
    "featureData = MinMaxScaler().fit_transform(featureData) # 缩放到[0, 1]\n",
    "train_x, test_x, train_y, test_y = train_test_split(featureData, featureLabel, test_size=0.2)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239c8aea-2509-455c-aab1-bc3d944dfe2d",
   "metadata": {},
   "source": [
    "新测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e867d06e-f51b-4486-9b57-430063692443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10062\\AppData\\Local\\Temp\\ipykernel_17508\\3774368366.py:133: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  sorted_data = sorted(dataset, key=lambda x: x[feature])\n",
      "C:\\Users\\10062\\AppData\\Local\\Temp\\ipykernel_17508\\3774368366.py:137: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  left_data = np.array([row for row in sorted_data if row[feature] <= threshold])\n",
      "C:\\Users\\10062\\AppData\\Local\\Temp\\ipykernel_17508\\3774368366.py:138: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  right_data = np.array([row for row in sorted_data if row[feature] > threshold])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一棵树生成所用时间 5025.003315s!\n",
      "2\n",
      "一棵树生成所用时间 3794.260574s!\n",
      "3\n",
      "一棵树生成所用时间 3754.897275s!\n",
      "4\n",
      "一棵树生成所用时间 3649.704136s!\n",
      "5\n",
      "一棵树生成所用时间 3646.613616s!\n",
      "6\n",
      "一棵树生成所用时间 3893.775812s!\n",
      "7\n",
      "一棵树生成所用时间 3743.324507s!\n",
      "8\n",
      "一棵树生成所用时间 3403.643856s!\n",
      "9\n",
      "一棵树生成所用时间 3300.691520s!\n",
      "10\n",
      "一棵树生成所用时间 3841.123628s!\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "RF = RandomForest(num_estimators = 10, \n",
    "                  max_depth = 12, min_samples_split = 1, numberSamplePerClass = 100)\n",
    "RF.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cb36d131-7c4f-4b53-a411-e7e04ec3464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10062\\AppData\\Local\\Temp\\ipykernel_17508\\3774368366.py:200: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if point.size > 0 and point[feature] <= threshold:                            #比较当前维度下未知点和阈值的大小，判断走左子树还是右子树\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5644146034410407\n",
      "10棵树/每个动作100个样本/最大深度12，花费时间: 41517.84 秒。\n",
      "RF test  accuracy: 56.44%\n",
      "0.6766341412233764\n",
      "0.6766341412233764\n",
      "10棵树/每个动作100个样本/最大深度12，花费时间: 41518.23 秒。\n",
      "RF train  accuracy: 67.66%\n"
     ]
    }
   ],
   "source": [
    "predictTest = RF.predict(test_x)\n",
    "\n",
    "predictnew1 = predictTest.reshape(len(predictTest))\n",
    "count1=0\n",
    "for i in range(len(predictnew1)):\n",
    "    if predictnew1[i] is None:\n",
    "        #print(i,predictnew[i])\n",
    "        count1=count1+1\n",
    "        predictnew1[i]=0\n",
    "count1=0\n",
    "for i in range(len(predictnew1)):\n",
    "    if predictnew1[i] == test_y[i]:\n",
    "        count1 = count1+1\n",
    "\n",
    "count1 = count1/(len(predictnew1))\n",
    "print(count1)\n",
    "print('10棵树/每个动作100个样本/最大深度12，花费时间: %.2f 秒。' % (time.time() - start))\n",
    "print('RF test  accuracy: %.2f%%' % (100 * count1))\n",
    "\n",
    "predictTrain = RF.predict(train_x)\n",
    "\n",
    "predictnew2 = predictTrain.reshape(len(predictTrain))\n",
    "count2=0\n",
    "for i in range(len(predictnew2)):\n",
    "    if predictnew2[i] is None:\n",
    "        #print(i,predictnew[i])\n",
    "        count2=count2+1\n",
    "        predictnew2[i]=0\n",
    "print(count)\n",
    "count2=0\n",
    "for i in range(len(predictnew2)):\n",
    "    if predictnew2[i] == train_y[i]:\n",
    "        count2 = count2+1\n",
    "\n",
    "count2 = count2/(len(predictnew2))\n",
    "print(count2)\n",
    "print('10棵树/每个动作100个样本/最大深度12，花费时间: %.2f 秒。' % (time.time() - start))\n",
    "print('RF train  accuracy: %.2f%%' % (100 * count2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe782a5-8572-4483-a095-fd7bc49cd484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bff4f15-5d94-4136-8d6a-dd3ae4dc6277",
   "metadata": {},
   "source": [
    "对比研究，以下是sklearn中的随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23d03e7-65a5-4303-8549-d75a71b9956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "\n",
    "file = h5py.File('DB2//DB2_S1_feature_200_0.h5','r')\n",
    "featureData   = file['featureData'][:]\n",
    "featureLabel  = file['featureLabel'][:]\n",
    "file.close()\n",
    "\n",
    "featureData = MinMaxScaler().fit_transform(featureData) # 缩放到[0, 1]\n",
    "trainData, testData, trainLabel, testLabel = train_test_split(featureData, featureLabel, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6afe15f-066f-4658-8199-b150ad434af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train accuracy: 99.63%\n",
      "RF test  accuracy: 74.99%\n",
      "training took 0.855669s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:615: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "RF2 = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2, \n",
    "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt',\n",
    "                            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                            bootstrap=True, oob_score=True, n_jobs=1, random_state=None, verbose=0,\n",
    "                            warm_start=False, class_weight=None)\n",
    "\n",
    "\n",
    "RF2.fit(trainData, trainLabel)\n",
    "score2 = RF2.score(trainData, trainLabel)\n",
    "predict2 = RF2.predict(testData)\n",
    "accuracy2 = metrics.accuracy_score(testLabel, predict2)\n",
    "\n",
    "print(\"RF train accuracy: %.2f%%\" %(100*score2))\n",
    "print('RF test  accuracy: %.2f%%' % (100 * accuracy2))\n",
    "print ('training took %fs!' % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7970a5-c8b8-42ba-92fe-85b757da96f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
